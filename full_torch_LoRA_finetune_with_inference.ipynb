{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UhSGggEqDKxh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "from transformers import Qwen2Tokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMhwDUzMDpQI",
        "outputId": "2687bd54-e4de-483f-c2df-405e28d88af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "torch.float16\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "dtype = torch.float16\n",
        "\n",
        "print(dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvuaUtdDECAF",
        "outputId": "578002ea-2c0a-431e-b9e5-46745c9c924c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:86: UserWarning: \n",
            "Access to the secret `HF_TOKEN` has not been granted on this notebook.\n",
            "You will not be requested again.\n",
            "Please restart the session if you want to be prompted again.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "# model_name = \"Qwen/Qwen3-4B-Instruct-2507\" у кого карточки есть покруче!\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    dtype=torch.float32\n",
        ").to(device)\n",
        "\n",
        "tokenizer = Qwen2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.enable_input_require_grads()\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q9D8A4G0Ehtj"
      },
      "outputs": [],
      "source": [
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ojIdNw57FD_4"
      },
      "outputs": [],
      "source": [
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, original_linear: nn.Linear, rank: int, alpha: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.register_buffer('weight', original_linear.weight.data)\n",
        "        if original_linear.bias is not None:\n",
        "            self.register_buffer('bias', original_linear.bias.data)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        feature_in = original_linear.in_features\n",
        "        feature_out = original_linear.out_features\n",
        "\n",
        "        self.scaling = alpha / rank\n",
        "\n",
        "        factory_kwargs = {'device': original_linear.weight.device, 'dtype': original_linear.weight.dtype}\n",
        "\n",
        "        self.lora_A = nn.Parameter(torch.empty((rank, feature_in), **factory_kwargs))\n",
        "        self.lora_B = nn.Parameter(torch.zeros((feature_out, rank), **factory_kwargs))\n",
        "\n",
        "        self.lora_A.requires_grad = True\n",
        "        self.lora_B.requires_grad = True\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        original_out = F.linear(x, self.weight, self.bias)\n",
        "\n",
        "        lora_out = (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
        "\n",
        "        return original_out + lora_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dobEwgh3w0vQ"
      },
      "outputs": [],
      "source": [
        "def inject_lora(model, rank=32, alpha=64, target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]):\n",
        "    \"\"\"\n",
        "    Доступные: \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\".\n",
        "    \"\"\"\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if any(target in name for target in target_modules):\n",
        "                lora_layer = LoRALinear(module, rank, alpha)\n",
        "                setattr(model, name, lora_layer)\n",
        "        else:\n",
        "            inject_lora(module, rank, alpha, target_modules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0ymAtJ9b1x-V"
      },
      "outputs": [],
      "source": [
        "inject_lora(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ3w3w_z11y4",
        "outputId": "c4d32d6c-fd67-4704-e146-03b99fc18097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Обучаемых параметров: 9,175,040 (2.14%)\n"
          ]
        }
      ],
      "source": [
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nОбучаемых параметров: {trainable_params:,} ({trainable_params/all_params:.2%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g-JvQf8xUf-p"
      },
      "outputs": [],
      "source": [
        "class GSM8kDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "            {\"role\": \"assistant\", \"content\": answer}\n",
        "        ]\n",
        "\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=False,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len\n",
        "        )\n",
        "\n",
        "        user_messages = [{\"role\": \"user\", \"content\": question}]\n",
        "        prompt_ids = self.tokenizer.apply_chat_template(\n",
        "            user_messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        prompt_len = len(prompt_ids)\n",
        "\n",
        "        labels = list(input_ids)\n",
        "\n",
        "        mask_len = min(prompt_len, len(labels))\n",
        "\n",
        "        for i in range(mask_len):\n",
        "            labels[i] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": [1] * len(input_ids),\n",
        "            \"labels\": labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "__bF95m9ZKYG"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
        "\n",
        "    input_ids_list, attention_mask_list, labels_list = [], [], []\n",
        "\n",
        "    for x in batch:\n",
        "        inp = x[\"input_ids\"]\n",
        "        mask = x[\"attention_mask\"]\n",
        "        lab = x[\"labels\"]\n",
        "\n",
        "        pad_len = max_len - len(inp)\n",
        "\n",
        "        padded_inp = [tokenizer.pad_token_id] * pad_len + inp\n",
        "        padded_mask = [0] * pad_len + mask\n",
        "        padded_lab = [-100] * pad_len + lab\n",
        "\n",
        "        input_ids_list.append(torch.tensor(padded_inp))\n",
        "        attention_mask_list.append(torch.tensor(padded_mask))\n",
        "        labels_list.append(torch.tensor(padded_lab))\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": torch.stack(input_ids_list),\n",
        "        \"attention_mask\": torch.stack(attention_mask_list),\n",
        "        \"labels\": torch.stack(labels_list)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oQ3iwdoXZQh8"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
        "train_ds = GSM8kDataset(dataset, tokenizer, 1024)\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IOwu5RMqaGaF"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4,\n",
        "    fused=torch.cuda.is_available()\n",
        ")\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=(dtype == torch.float16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7bMrNwpNasn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3824ce74-1c6f-4505-dda1-c52a138d3f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0 | Loss: 0.4580:  50%|█████     | 1869/3738 [16:44<14:21,  2.17it/s]"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "step = 0\n",
        "total_loss = 0\n",
        "accumulation_steps = 16\n",
        "progress_bar = tqdm(range(len(train_loader)))\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "for epoch in range(1):\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with torch.amp.autocast('cuda', dtype=dtype):\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            loss = outputs.loss / accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                max_norm=1.0\n",
        "            )\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.set_description(f\"Epoch {epoch} | Loss: {total_loss:.4f}\")\n",
        "            total_loss = 0\n",
        "\n",
        "        step += 1\n",
        "        progress_bar.update(1)\n",
        "\n",
        "if step % accumulation_steps != 0:\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        max_norm=1.0\n",
        "    )\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "lora_state_dict = {k: v.cpu() for k, v in model.state_dict().items() if \"lora_\" in k}\n",
        "output_path = \"custom_lora.pt\"\n",
        "torch.save(lora_state_dict, output_path)\n",
        "\n",
        "print(f\"Готово! Адаптер сохранен в '{output_path}'.\")\n",
        "print(f\"Размер файла: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i0xHt1R42I7",
        "outputId": "30144a48-b604-449a-8653-6ee205df4ea9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Готово! Адаптер сохранен в 'custom_lora.pt'.\n",
            "Размер файла: 35.07 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference(можно перезапустить ноутбук и начать отсюда, сохранив адаптер)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ftCflP-J4xCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "R2x4Kaq2cKom"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, Qwen2Tokenizer, GenerationConfig\n",
        "import math\n",
        "\n",
        "#КОНФИГУРАЦИЯ (должна совпадать с обучением!)\n",
        "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
        "LORA_PATH = \"/content/custom_lora.pt\"\n",
        "RANK = 32\n",
        "ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, original_linear: nn.Linear, rank: int, alpha: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.register_buffer('weight', original_linear.weight.data)\n",
        "        if original_linear.bias is not None:\n",
        "            self.register_buffer('bias', original_linear.bias.data)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        feature_in = original_linear.in_features\n",
        "        feature_out = original_linear.out_features\n",
        "\n",
        "        self.scaling = alpha / rank\n",
        "\n",
        "        factory_kwargs = {'device': original_linear.weight.device, 'dtype': original_linear.weight.dtype}\n",
        "\n",
        "        self.lora_A = nn.Parameter(torch.empty((rank, feature_in), **factory_kwargs))\n",
        "        self.lora_B = nn.Parameter(torch.zeros((feature_out, rank), **factory_kwargs))\n",
        "\n",
        "        self.lora_A.requires_grad = True\n",
        "        self.lora_B.requires_grad = True\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        original_out = F.linear(x, self.weight, self.bias)\n",
        "\n",
        "        lora_out = (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
        "\n",
        "        return original_out + lora_out"
      ],
      "metadata": {
        "id": "cr7-Wci-6Esn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inject_lora(model, rank=32, alpha=64, target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]):\n",
        "    \"\"\"\n",
        "    Доступные: \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\".\n",
        "    \"\"\"\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if any(target in name for target in target_modules):\n",
        "                lora_layer = LoRALinear(module, rank, alpha)\n",
        "                setattr(model, name, lora_layer)\n",
        "        else:\n",
        "            inject_lora(module, rank, alpha, target_modules)"
      ],
      "metadata": {
        "id": "aZ2FNW_M6KJD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    dtype=torch.float16\n",
        ").to(device)\n",
        "model.config.use_cache = True\n",
        "\n",
        "tokenizer = Qwen2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "FlsRBc086Y_w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inject_lora(model, RANK, ALPHA, TARGET_MODULES)"
      ],
      "metadata": {
        "id": "w4jIYXqv6nvv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(LORA_PATH, map_location=device)\n",
        "missing, unexpected = model.load_state_dict(state_dict, strict=False)"
      ],
      "metadata": {
        "id": "xoM5nFKs6sMs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if any(\"lora_\" in k for k in missing):\n",
        "    print(\"Какие-то веса LoRA не найдены! Проверь названия слоев.\")\n",
        "else:\n",
        "    print(\"Успех! Адаптеры загружены.\")\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhBv8W006s0C",
        "outputId": "ed84b06e-4915-41a3-db82-46161b44deb5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Успех! Адаптеры загружены.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen3ForCausalLM(\n",
              "  (model): Qwen3Model(\n",
              "    (embed_tokens): Embedding(151936, 1024)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen3DecoderLayer(\n",
              "        (self_attn): Qwen3Attention(\n",
              "          (q_proj): LoRALinear()\n",
              "          (k_proj): LoRALinear()\n",
              "          (v_proj): LoRALinear()\n",
              "          (o_proj): LoRALinear()\n",
              "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Qwen3MLP(\n",
              "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
              "    (rotary_emb): Qwen3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Natalia has 5 apples. She buys 3 packs of 4 apples. How many apples?\"\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": question}]\n",
        "input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
        "print(f\"\\nВопрос: {question}\")\n",
        "print(f\"Ответ: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjOU_AKc674s",
        "outputId": "2f7ebd5d-4b70-46d1-bdd9-9c44ac63be92"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Вопрос: Natalia has 5 apples. She buys 3 packs of 4 apples. How many apples?\n",
            "Ответ: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "She buys 3*4=<<3*4=12>>12 apples.\n",
            "So Natalia has 5+12=<<5+12=17>>17 apples.\n",
            "#### 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xn_43aRw7MRB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}